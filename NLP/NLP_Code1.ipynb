{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3906fac5-97c4-4fcf-aaee-5c8fdb3e04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86775b75-0d26-4fc0-9018-6ab26ddb74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a968b8-76af-468c-ac82-236ff4c8ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f19d53e-0774-4329-88d7-a73905b12de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences split : ['Hello Sreenivasa.', 'How are you?', \"I hope you're learning NLP.\", 'This is nice!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello Sreenivasa. How are you? I hope you're learning NLP. This is nice!\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences split :\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc2f3ea-78b8-4500-90d8-ffea0f695900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'in', 'Artificial', 'Intelligence', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is an exciting field in Artificial Intelligence!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0f0d97-6cd3-4c4f-9679-649d1a011b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: {'couldn', 'was', \"it'd\", 'have', \"she'll\", \"that'll\", 'ain', 'i', 'needn', 'same', 'whom', \"she's\", 'mightn', 'most', 'that', 'their', \"we'd\", \"we've\", 'in', 'can', 'won', 'were', 'over', 'very', 'before', \"needn't\", 'where', 'she', \"you've\", 'y', \"wasn't\", 'under', 'doing', 'each', 'are', 'at', \"weren't\", 'll', 'both', \"haven't\", 'own', 'then', 'him', 'hasn', 'o', 'only', 'while', \"he's\", 'being', 'up', \"won't\", 'those', 'we', 'you', 'by', \"mustn't\", 'until', 'some', 'these', 'to', 'aren', 'yourselves', \"i'm\", 'will', 'not', 'again', \"i've\", 'do', \"we'll\", 's', 'be', 'because', \"should've\", 'from', 't', 'more', 'other', 'for', 'has', 'herself', 'yourself', \"you'll\", 'with', 'doesn', 'ourselves', 'so', \"they'd\", \"wouldn't\", 'haven', 'wasn', 'out', 'but', 'after', 'is', 'why', 'which', 'down', 'the', 'when', 'her', 'about', 'didn', 'had', 'between', \"hasn't\", 've', 'ma', 'does', 'he', \"shouldn't\", \"they'll\", \"don't\", 'there', 'if', 'been', \"it'll\", 'any', 'shan', 'his', \"doesn't\", 'theirs', 'off', 'hers', 'having', 'my', 'few', 'shouldn', 'themselves', 'weren', 'itself', 'below', \"we're\", 'how', 'isn', 'did', 'they', 'it', \"mightn't\", 'should', 'what', 'into', 'who', 'as', 'during', 'yours', 'here', 'am', 'too', 'myself', 'just', 'no', 'me', \"couldn't\", 'or', 'mustn', 'on', 'ours', \"they're\", \"shan't\", 'hadn', 'above', \"i'd\", \"aren't\", \"isn't\", 'himself', 'this', 'than', 'and', \"didn't\", 'a', \"it's\", \"he'll\", 'an', 'now', \"hadn't\", 'm', 'don', \"he'd\", 'such', 'against', 'wouldn', 'its', \"she'd\", \"you'd\", 'of', 'once', 'further', \"they've\", 'all', 're', \"you're\", 'through', 'd', 'your', 'nor', \"i'll\", 'them', 'our'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(\"stop_words:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4af551-4aa2-4f86-9a16-cb060f7e694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Natural', 'Language', 'Processing', 'exciting', 'field', 'Artificial', 'Intelligence', '!']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "filtered = [word for word in tokens if word.lower() not in stop_words] # it is combination of for and if conditions.\n",
    "print(\"Filtered Tokens:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c8bf0cf-937a-40bb-a508-78a16bb0ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running → run\n",
      "runs → run\n",
      "runner → runner\n",
      "easily → easili\n",
      "fairly → fairli\n",
      "happiness → happi\n"
     ]
    }
   ],
   "source": [
    "# Stemmer (Most common)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\", \"happiness\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"→\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba2813a-c91a-4fa8-a93b-24e3e579c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running      → run\n",
      "runner       → runner\n",
      "easily       → easili\n",
      "fairly       → fairli\n",
      "happiness    → happi\n",
      "flying       → fli\n",
      "cars         → car\n"
     ]
    }
   ],
   "source": [
    "#1. Porter Stemmer (Classic & widely used)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"easily\", \"fairly\", \"happiness\", \"flying\", \"cars\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2aa805a-c955-491f-bb32-8f8ef09dbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing      → play\n",
      "plays        → play\n",
      "played       → play\n",
      "automation   → autom\n",
      "automated    → autom\n"
     ]
    }
   ],
   "source": [
    "#2. Snowball Stemmer (Better & cleaner version)\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"playing\", \"plays\", \"played\", \"automation\", \"automated\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {snow.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8be16ec6-539c-43a5-9ab1-f1081f972812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running      → run\n",
      "runner       → run\n",
      "nationality  → nat\n",
      "happiness    → happy\n",
      "maximum      → maxim\n",
      "taking       → tak\n"
     ]
    }
   ],
   "source": [
    "# 3. Lancaster Stemmer (Very aggressive)\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"nationality\", \"happiness\", \"maximum\", \"taking\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {ls.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1018012a-546e-4f47-a628-95683b584b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The          → The\n",
      "children     → child\n",
      "are          → are\n",
      "running      → running\n",
      "and          → and\n",
      "the          → the\n",
      "cats         → cat\n",
      "were         → were\n",
      "eating       → eating\n",
      "their        → their\n",
      "meals        → meal\n",
      ".            → .\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer Code in Python (NLTK — simple version)\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"The children are running and the cats were eating their meals.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for w in tokens:\n",
    "    print(f\"{w:12} → {lemmatizer.lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "159ed21e-2010-4e8b-a55f-d3d950674fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['child', 'eating', 'apple', 'happily', 'cat', 'running']\n"
     ]
    }
   ],
   "source": [
    "# Stopwords + Lemmatizer in Pypthon\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords + lemmatizes words.\n",
    "    No POS tagging (simple lemmatizer).\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    result = []\n",
    "    for w in tokens:\n",
    "        w_lower = w.lower()\n",
    "\n",
    "        # skip stopwords & non-alphabetic words\n",
    "        if w_lower in stop_words or not w_lower.isalpha():\n",
    "            continue\n",
    "\n",
    "        # lemmatize\n",
    "        lemma = lemmatizer.lemmatize(w_lower)\n",
    "        result.append(lemma)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"The children were eating apples happily while the cats were running.\"\n",
    "\n",
    "output = clean_text(text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccd2fe6d-2e41-46d9-bd9c-e6d171b7bfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging Code in Python\n",
    "\n",
    "import nltk\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07e5cfc5-83ae-4a18-99aa-3c421f74f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: children      POS: NNS     LEMMA: child\n",
      "WORD: eating        POS: VBG     LEMMA: eat\n",
      "WORD: apples        POS: NNS     LEMMA: apple\n",
      "WORD: happily       POS: RB      LEMMA: happily\n",
      "WORD: cats          POS: NNS     LEMMA: cat\n",
      "WORD: running       POS: VBG     LEMMA: run\n",
      "WORD: fast          POS: RB      LEMMA: fast\n"
     ]
    }
   ],
   "source": [
    "# FULL NLP PIPELINE CODE (Token → Stopwords → POS → Lemma)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Helper to map POS tags → WordNet POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default\n",
    "\n",
    "\n",
    "def nlp_pipeline(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    final_words = []\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        w = word.lower()\n",
    "\n",
    "        # Skip stopwords & non-alphabetic\n",
    "        if w in stop_words or not w.isalpha():\n",
    "            continue\n",
    "\n",
    "        # Lemmatize using POS\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(w, wn_pos)\n",
    "\n",
    "        final_words.append({\n",
    "            \"word\": word,\n",
    "            \"pos\": tag,\n",
    "            \"lemma\": lemma\n",
    "        })\n",
    "\n",
    "    return final_words\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"The children are eating apples happily while the cats were running fast.\"\n",
    "\n",
    "output = nlp_pipeline(text)\n",
    "\n",
    "for item in output:\n",
    "    print(f\"WORD: {item['word']:12}  POS: {item['pos']:6}  LEMMA: {item['lemma']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1bcf597-2cbb-454d-b357-fc215f12dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Word: 'run' ===\n",
      "\n",
      "Sentence: I run every morning.\n",
      "Tokens + POS:\n",
      "  I            PRP   \n",
      "  run          VBP     <-- target\n",
      "  every        DT    \n",
      "  morning      NN    \n",
      "  .            .     \n",
      "\n",
      "Sentence: He went for a run.\n",
      "Tokens + POS:\n",
      "  He           PRP   \n",
      "  went         VBD   \n",
      "  for          IN    \n",
      "  a            DT    \n",
      "  run          NN      <-- target\n",
      "  .            .     \n",
      "----------------------------------------\n",
      "\n",
      "=== Word: 'light' ===\n",
      "\n",
      "Sentence: This bag is light.\n",
      "Tokens + POS:\n",
      "  This         DT    \n",
      "  bag          NN    \n",
      "  is           VBZ   \n",
      "  light        JJ      <-- target\n",
      "  .            .     \n",
      "\n",
      "Sentence: Turn on the light.\n",
      "Tokens + POS:\n",
      "  Turn         NN    \n",
      "  on           IN    \n",
      "  the          DT    \n",
      "  light        NN      <-- target\n",
      "  .            .     \n",
      "\n",
      "Sentence: Please light the candle.\n",
      "Tokens + POS:\n",
      "  Please       NNP   \n",
      "  light        VBD     <-- target\n",
      "  the          DT    \n",
      "  candle       NN    \n",
      "  .            .     \n",
      "----------------------------------------\n",
      "\n",
      "=== Word: 'book' ===\n",
      "\n",
      "Sentence: I read a book.\n",
      "Tokens + POS:\n",
      "  I            PRP   \n",
      "  read         VBP   \n",
      "  a            DT    \n",
      "  book         NN      <-- target\n",
      "  .            .     \n",
      "\n",
      "Sentence: I will book a ticket.\n",
      "Tokens + POS:\n",
      "  I            PRP   \n",
      "  will         MD    \n",
      "  book         NN      <-- target\n",
      "  a            DT    \n",
      "  ticket       NN    \n",
      "  .            .     \n",
      "----------------------------------------\n",
      "\n",
      "=== Word: 'watch' ===\n",
      "\n",
      "Sentence: I watch movies at night.\n",
      "Tokens + POS:\n",
      "  I            PRP   \n",
      "  watch        VBP     <-- target\n",
      "  movies       NNS   \n",
      "  at           IN    \n",
      "  night        NN    \n",
      "  .            .     \n",
      "\n",
      "Sentence: I bought a new watch yesterday.\n",
      "Tokens + POS:\n",
      "  I            PRP   \n",
      "  bought       VBD   \n",
      "  a            DT    \n",
      "  new          JJ    \n",
      "  watch        NN      <-- target\n",
      "  yesterday    NN    \n",
      "  .            .     \n",
      "----------------------------------------\n",
      "\n",
      "=== Word: 'park' ===\n",
      "\n",
      "Sentence: We went to the park.\n",
      "Tokens + POS:\n",
      "  We           PRP   \n",
      "  went         VBD   \n",
      "  to           TO    \n",
      "  the          DT    \n",
      "  park         NN      <-- target\n",
      "  .            .     \n",
      "\n",
      "Sentence: Please park the car here.\n",
      "Tokens + POS:\n",
      "  Please       NNP   \n",
      "  park         VB      <-- target\n",
      "  the          DT    \n",
      "  car          NN    \n",
      "  here         RB    \n",
      "  .            .     \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# show how the same word gets different POS tags in different sentences.\n",
    "\n",
    "#What this does\n",
    "\n",
    "#Tokenizes each sentence\n",
    "#Runs POS tagging\n",
    "#Prints each token with its POS tag\n",
    "#Marks the target word (<-- target) so you can see how its tag changes (e.g., VB vs NN, etc.)\n",
    "\n",
    "import nltk\n",
    "\n",
    "\n",
    "def show_pos(word, sentences):\n",
    "    print(f\"\\n=== Word: '{word}' ===\")\n",
    "    for s in sentences:\n",
    "        tokens = nltk.word_tokenize(s)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        print(f\"\\nSentence: {s}\")\n",
    "        print(\"Tokens + POS:\")\n",
    "        for tok, tag in tags:\n",
    "            mark = \"  <-- target\" if tok.lower() == word.lower() else \"\"\n",
    "            print(f\"  {tok:12} {tag:5} {mark}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Examples for different words\n",
    "run_sentences = [\n",
    "    \"I run every morning.\",\n",
    "    \"He went for a run.\",\n",
    "]\n",
    "\n",
    "light_sentences = [\n",
    "    \"This bag is light.\",\n",
    "    \"Turn on the light.\",\n",
    "    \"Please light the candle.\",\n",
    "]\n",
    "\n",
    "book_sentences = [\n",
    "    \"I read a book.\",\n",
    "    \"I will book a ticket.\",\n",
    "]\n",
    "\n",
    "watch_sentences = [\n",
    "    \"I watch movies at night.\",\n",
    "    \"I bought a new watch yesterday.\",\n",
    "]\n",
    "\n",
    "park_sentences = [\n",
    "    \"We went to the park.\",\n",
    "    \"Please park the car here.\",\n",
    "]\n",
    "\n",
    "# Show POS behavior\n",
    "show_pos(\"run\", run_sentences)\n",
    "show_pos(\"light\", light_sentences)\n",
    "show_pos(\"book\", book_sentences)\n",
    "show_pos(\"watch\", watch_sentences)\n",
    "show_pos(\"park\", park_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1401a292-86b9-4bcf-9157-f2f54efefaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Python Code — NLTK Chunking Example\n",
    "\n",
    "import nltk\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Step 2: POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Step 3: Chunk Grammar Rule (Noun Phrase)\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}   # NP = optional determiner + adjectives + noun\n",
    "\"\"\"\n",
    "\n",
    "# Step 4: Create chunk parser\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Step 5: Parse (chunking)\n",
    "chunk_tree = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(chunk_tree)\n",
    "chunk_tree.draw()   # graphical tree window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eceeadc-0800-41e1-ba23-d4e48e8ad112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767dda5-3449-4cfc-afc1-5ef2f16b2bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc670c76-5ef6-4870-a5f5-115468d01700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIVirtualEnv",
   "language": "python",
   "name": "aivirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
