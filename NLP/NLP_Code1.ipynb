{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3906fac5-97c4-4fcf-aaee-5c8fdb3e04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86775b75-0d26-4fc0-9018-6ab26ddb74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a968b8-76af-468c-ac82-236ff4c8ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f19d53e-0774-4329-88d7-a73905b12de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences split : ['Hello Sreenivasa.', 'How are you?', \"I hope you're learning NLP.\", 'This is nice!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello Sreenivasa. How are you? I hope you're learning NLP. This is nice!\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences split :\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc2f3ea-78b8-4500-90d8-ffea0f695900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'an', 'exciting', 'field', 'in', 'Artificial', 'Intelligence', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is an exciting field in Artificial Intelligence!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0f0d97-6cd3-4c4f-9679-649d1a011b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words: {'couldn', 'was', \"it'd\", 'have', \"she'll\", \"that'll\", 'ain', 'i', 'needn', 'same', 'whom', \"she's\", 'mightn', 'most', 'that', 'their', \"we'd\", \"we've\", 'in', 'can', 'won', 'were', 'over', 'very', 'before', \"needn't\", 'where', 'she', \"you've\", 'y', \"wasn't\", 'under', 'doing', 'each', 'are', 'at', \"weren't\", 'll', 'both', \"haven't\", 'own', 'then', 'him', 'hasn', 'o', 'only', 'while', \"he's\", 'being', 'up', \"won't\", 'those', 'we', 'you', 'by', \"mustn't\", 'until', 'some', 'these', 'to', 'aren', 'yourselves', \"i'm\", 'will', 'not', 'again', \"i've\", 'do', \"we'll\", 's', 'be', 'because', \"should've\", 'from', 't', 'more', 'other', 'for', 'has', 'herself', 'yourself', \"you'll\", 'with', 'doesn', 'ourselves', 'so', \"they'd\", \"wouldn't\", 'haven', 'wasn', 'out', 'but', 'after', 'is', 'why', 'which', 'down', 'the', 'when', 'her', 'about', 'didn', 'had', 'between', \"hasn't\", 've', 'ma', 'does', 'he', \"shouldn't\", \"they'll\", \"don't\", 'there', 'if', 'been', \"it'll\", 'any', 'shan', 'his', \"doesn't\", 'theirs', 'off', 'hers', 'having', 'my', 'few', 'shouldn', 'themselves', 'weren', 'itself', 'below', \"we're\", 'how', 'isn', 'did', 'they', 'it', \"mightn't\", 'should', 'what', 'into', 'who', 'as', 'during', 'yours', 'here', 'am', 'too', 'myself', 'just', 'no', 'me', \"couldn't\", 'or', 'mustn', 'on', 'ours', \"they're\", \"shan't\", 'hadn', 'above', \"i'd\", \"aren't\", \"isn't\", 'himself', 'this', 'than', 'and', \"didn't\", 'a', \"it's\", \"he'll\", 'an', 'now', \"hadn't\", 'm', 'don', \"he'd\", 'such', 'against', 'wouldn', 'its', \"she'd\", \"you'd\", 'of', 'once', 'further', \"they've\", 'all', 're', \"you're\", 'through', 'd', 'your', 'nor', \"i'll\", 'them', 'our'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(\"stop_words:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4af551-4aa2-4f86-9a16-cb060f7e694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Natural', 'Language', 'Processing', 'exciting', 'field', 'Artificial', 'Intelligence', '!']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "filtered = [word for word in tokens if word.lower() not in stop_words] # it is combination of for and if conditions.\n",
    "print(\"Filtered Tokens:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c8bf0cf-937a-40bb-a508-78a16bb0ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running → run\n",
      "runs → run\n",
      "runner → runner\n",
      "easily → easili\n",
      "fairly → fairli\n",
      "happiness → happi\n"
     ]
    }
   ],
   "source": [
    "# Stemmer (Most common)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\", \"happiness\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \"→\", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba2813a-c91a-4fa8-a93b-24e3e579c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running      → run\n",
      "runner       → runner\n",
      "easily       → easili\n",
      "fairly       → fairli\n",
      "happiness    → happi\n",
      "flying       → fli\n",
      "cars         → car\n"
     ]
    }
   ],
   "source": [
    "#1. Porter Stemmer (Classic & widely used)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"easily\", \"fairly\", \"happiness\", \"flying\", \"cars\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2aa805a-c955-491f-bb32-8f8ef09dbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing      → play\n",
      "plays        → play\n",
      "played       → play\n",
      "automation   → autom\n",
      "automated    → autom\n"
     ]
    }
   ],
   "source": [
    "#2. Snowball Stemmer (Better & cleaner version)\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "words = [\"playing\", \"plays\", \"played\", \"automation\", \"automated\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {snow.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8be16ec6-539c-43a5-9ab1-f1081f972812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running      → run\n",
      "runner       → run\n",
      "nationality  → nat\n",
      "happiness    → happy\n",
      "maximum      → maxim\n",
      "taking       → tak\n"
     ]
    }
   ],
   "source": [
    "# 3. Lancaster Stemmer (Very aggressive)\n",
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "words = [\"running\", \"runner\", \"nationality\", \"happiness\", \"maximum\", \"taking\"]\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w:12} → {ls.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018012a-546e-4f47-a628-95683b584b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIVirtualEnv",
   "language": "python",
   "name": "aivirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
